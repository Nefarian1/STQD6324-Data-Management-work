# -*- coding: utf-8 -*-
"""Assignment4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ld0l-Wpv7x3E0LPjlmqD9P5YNOrlajDH

# STQD6324 Assignment 4 PANZHANGYU P136922

Using the u.user file from the MovieLens 100k Dataset (ml-100k.zip), which can be
downloaded from https://grouplens.org/datasets/movielens/, write a python script that acts
as a wrapper function to execute Cassandra Query Language (CQL) and Spark2 Structured
Query Language (SQL) in order to answer the following questions [display only the top ten
results for each question]:
+ i) Calculate the average rating for each movie.
+ ii) Identify the top ten movies with the highest average ratings.
+ iii) Find the users who have rated at least 50 movies and identify their favourite movie
genres.
+ iv) Find all the users with age that is less than 20 years old.
+ v) Find all the users who have the occupation â€œscientistâ€ and their age is between 30 and
40 years old.

Your python script should include the following elements:
1. Python libraries used to execute Spark2 and Cassandra sessions.
2. Functions to parse the u.user file into HDFS.
3. Functions to load, read, and create Resilient Distributed Dataset (RDD) objects.
4. Functions to convert the RDD objects into DataFrame.
5. Functions to write the DataFrame into the Keyspace database created in Cassandra.
6. Functions to read the table back from Cassandra into a new DataFrame.

#<font color='red'>My Files LocationğŸ¤“ğŸ¤“ğŸ¤“
The files I need are all in the Github homepage. Here is the corresponding link: https://github.com/Nefarian1/STQD6324-Data-Management-work/tree/f3a92d8eeece0d841ab4d4774089838ba0eda83e/assignment4

<font color='red'>assignment4_pzy_code.txt

# Introduce
+ u.data     -- The full u data set, 100000 ratings by 943 users on 1682 items.
              Each user has rated at least 20 movies.  Users and items are
              numbered consecutively from 1.  The data is randomly
              ordered. This is a tab separated list of
	         user id | item id | rating | timestamp.
              The time stamps are unix seconds since 1/1/1970 UTC   

+ u.info     -- The number of users, items, and ratings in the u data set.

+ u.item     -- Information about the items (movies); this is a tab separated
              list of
              movie id | movie title | release date | video release date |
              IMDb URL | unknown | Action | Adventure | Animation |
              Children's | Comedy | Crime | Documentary | Drama | Fantasy |
              Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |
              Thriller | War | Western |
              The last 19 fields are the genres, a 1 indicates the movie
              is of that genre, a 0 indicates it is not; movies can be in
              several genres at once.
              The movie ids are the ones used in the u.data data set.

+ u.genre    -- A list of the genres.

+ u.user     -- Demographic information about the users; this is a tab
              separated list of
              user id | age | gender | occupation | zip code
              The user ids are the ones used in the u.data data set.

+ u.occupation -- A list of the occupations.

# Prepare

cd ml-100k/

wget http://media.sundog-soft.com/hadoop/ml-100k/u.data

wget http://media.sundog-soft.com/hadoop/ml-100k/u.info

wget http://media.sundog-soft.com/hadoop/ml-100k/u.item

wget http://media.sundog-soft.com/hadoop/ml-100k/u.genre

wget http://media.sundog-soft.com/hadoop/ml-100k/u.user

wget http://media.sundog-soft.com/hadoop/ml-100k/u.occupation

ls
"""

from IPython.display import Image
Image('/content/ls.png', width = 900)

"""cd ..

su root

 + hadoop (password) -> panzhangyu

sudo yum install python-pip

sudo pip install numpy==1.16

 + ESC

 :1,$d æ¸…é™¤ Clear

 i æ’å…¥æ¨¡å¼ Insert Mode

# i Calculate the average rating for each movie

# i è®¡ç®—æ¯éƒ¨ç”µå½±çš„å¹³å‡è¯„åˆ†


vi oneSQL.py


-# -*- coding: utf-8 -*-

from pyspark.sql import SparkSession

from pyspark.sql import functions as F

def parseRating(line):

    fields = line.split('\t')
    return (int(fields[0]), int(fields[1]), float(fields[2]), int(fields[3]))

if __name__ == "__main__":
    # Creating a SparkSession
    spark = SparkSession.builder \
        .appName("AverageMovieRatings") \
        .getOrCreate()

    # è¯»å–æ•°æ® Reading Data
    lines = spark.sparkContext.textFile("hdfs:///user/maria_dev/ml-100k/u.data")
    
    # è§£ææ•°æ® Analytical data
    ratings = lines.map(parseRating)
    
    # Convert to DataFrame
    ratingsDF = ratings.toDF(["userID", "movieID", "rating", "timestamp"])
    
    # è®¡ç®—æ¯éƒ¨ç”µå½±çš„å¹³å‡è¯„åˆ† Calculate the average rating for each movie
    avgRatings = ratingsDF.groupBy("movieID").agg(F.avg("rating").alias("avg_rating"))
    
    #
    avgRatings.show(10)
    
    #
    spark.stop()



spark-submit oneSQL.py
"""

from IPython.display import Image
Image('/content/i.png', width = 400)

"""# ii Find the users who have rated at least 50 movies and their favorite movie genres

# ii æ‰¾å‡ºè‡³å°‘è¯„ä»·äº†50éƒ¨ç”µå½±çš„ç”¨æˆ·åŠå…¶æœ€å–œæ¬¢çš„ç”µå½±ç±»å‹

vi twoSQL.py


-# -*- coding: utf-8 -*-

from pyspark.sql import SparkSession

from pyspark.sql import functions as F

from pyspark.sql.types import IntegerType

if __name__ == "__main__":
    # Creating a SparkSession
    spark = SparkSession.builder \
        .appName("TopTenMovies") \
        .getOrCreate()

    # è¯»å–å¹¶è§£æç”µå½±è¯„åˆ†æ•°æ® Read and parse movie ratings data
    ratings = spark.read.csv("hdfs:///user/maria_dev/ml-100k/u.data", sep="\t")
    # æŒ‡å®šåˆ—çš„æ•°æ®ç±»å‹ä¸ºæ•´æ•° Specifies that the data type of the column is an integer
    ratings = ratings.withColumnRenamed("_c0", "userID") \
                     .withColumnRenamed("_c1", "movieID") \
                     .withColumnRenamed("_c2", "rating") \
                     .withColumnRenamed("_c3", "timestamp")
    ratings = ratings.select("movieID", ratings["rating"].cast(IntegerType()))

    # è®¡ç®—æ¯éƒ¨ç”µå½±çš„å¹³å‡è¯„åˆ†å¹¶æŒ‰é™åºæ’åˆ— Calculate the average rating for each movie and sort them in descending order
    average_ratings = ratings.groupBy("movieID").avg("rating")
    top_ten_movies = average_ratings.orderBy(F.desc("avg(rating)")).limit(10)

    #
    top_ten_movies.show()

    #
    spark.stop()


spark-submit twoSQL.py
"""

from IPython.display import Image
Image('/content/ii.png', width = 400)

"""# iii Find the users who have rated at least 50 movies and their favorite movie genres

# iii æ‰¾å‡ºè‡³å°‘è¯„ä»·äº†50éƒ¨ç”µå½±çš„ç”¨æˆ·åŠå…¶æœ€å–œæ¬¢çš„ç”µå½±ç±»å‹

vi threeSQL.py


-# -*- coding: utf-8 -*-

from pyspark.sql import SparkSession

from pyspark.sql import functions as F

def parseUserRating(line):
    fields = line.split('\t')
    return (int(fields[0]), int(fields[1]), float(fields[2]))

def parseMovie(line):
    fields = line.split('|')
    genres = [int(genre) for genre in fields[5:]]
    return (int(fields[0]), genres)

if __name__ == "__main__":
    #
    spark = SparkSession.builder \
        .appName("FrequentRaters") \
        .getOrCreate()

    #
    lines = spark.sparkContext.textFile("hdfs:///user/maria_dev/ml-100k/u.data")
    userRatings = lines.map(parseUserRating)
    userRatingsDF = userRatings.toDF(["userID", "movieID", "rating"])

    #
    movieLines = spark.sparkContext.textFile("hdfs:///user/maria_dev/ml-100k/u.item")
    movies = movieLines.map(parseMovie)
    moviesDF = movies.toDF(["movieID", "genres"])

    #
    moviesDF = moviesDF.select("movieID", F.explode("genres").alias("genreIndex"))

    #
    genres = ["unknown", "Action", "Adventure", "Animation", "Children's", "Comedy", "Crime",
              "Documentary", "Drama", "Fantasy", "Film-Noir", "Horror", "Musical", "Mystery",
              "Romance", "Sci-Fi", "Thriller", "War", "Western"]

    genreMapping = {i: genres[i] for i in range(len(genres))}

    # ç­›é€‰å‡ºè‡³å°‘å¯¹ 50 éƒ¨ç”µå½±è¿›è¡Œè¿‡è¯„åˆ†çš„ç”¨æˆ· Filter for users who have rated at least 50 movies
    userCounts = userRatingsDF.groupBy("userID").count().filter("count >= 50").select("userID")

    # æ‰¾å‡ºè¿™äº›ç”¨æˆ·æœ€å–œæ¬¢çš„ç”µå½±ç±»å‹ Find out the most popular movie genres for these users
    userRatingsDF.createOrReplaceTempView("userRatings")
    moviesDF.createOrReplaceTempView("movies")
    userCounts.createOrReplaceTempView("activeUsers")

    frequentRaters = spark.sql('''
        SELECT u.userID, m.genreIndex, COUNT(*) AS genre_count
        FROM userRatings u
        JOIN activeUsers a ON u.userID = a.userID
        JOIN movies m ON u.movieID = m.movieID
        WHERE m.genreIndex = 1
        GROUP BY u.userID, m.genreIndex
        ORDER BY genre_count DESC
    ''')

    # å°†ç”µå½±ç±»å‹ç´¢å¼•æ˜ å°„ä¸ºç±»å‹åç§° Map movie genre index to genre name
    frequentRaters = frequentRaters.withColumn("genre", F.udf(lambda index: genreMapping[index])(frequentRaters["genreIndex"]))

    #
    frequentRaters.select("userID", "genre", "genre_count").show(10)

    #
    spark.stop()


spark-submit threeSQL.py
"""

from IPython.display import Image
Image('/content/iii.png', width = 400)

"""# iv Find all users under 20 years old

# iv æ‰¾å‡ºæ‰€æœ‰å°äº20å²çš„ç”¨æˆ·

vi fourSQL.py


-# -*- coding: utf-8 -*-

from pyspark.sql import SparkSession

from pyspark.sql import Row

def parseUser(line):
    fields = line.split('|')
    return Row(userID=int(fields[0]), age=float(fields[1]), gender=fields[2], occupation=fields[3], zip=fields[4])

if __name__ == "__main__":
    # åˆ›å»ºä¸€ä¸ªSparkSession
    spark = SparkSession.builder \
    .appName("UsersUnder20") \
    .getOrCreate()
    
    #
    lines = spark.sparkContext.textFile("hdfs:///user/maria_dev/ml-100k/u.user")
    
    #
    users = lines.map(parseUser)
    
    # DataFrame
    userDataset = spark.createDataFrame(users)
    
    # æ‰¾å‡ºæ‰€æœ‰å°äº20å²çš„ç”¨æˆ·ã€‚ Find all users who are younger than 20 years old.
    usersUnder20 = userDataset.filter(userDataset.age < 20)
    
    #
    usersUnder20.show(10)
    
    #
    spark.stop()


spark-submit fourSQL.py
"""

from IPython.display import Image
Image('/content/iv.png', width = 400)

"""# v Find users whose occupation is "scientist" and whose age is between 30 and 40

# v æ‰¾å‡ºèŒä¸šæ˜¯â€œscientistâ€ï¼Œå¹´é¾„åœ¨30åˆ°40å²ä¹‹é—´çš„ç”¨æˆ·

vi fiveSQL.py


-# -*- coding: utf-8 -*-

from pyspark.sql import SparkSession

from pyspark.sql import functions as F

from collections import namedtuple

-# å®šä¹‰å‘½åå…ƒç»„ Defining a named tuple

User = namedtuple("User", ["userID", "age", "gender", "occupation", "zip"])


def parseUser(line):

    try:

        fields = line.split('|')
        return User(userID=int(fields[0]), age=int(fields[1]), gender=fields[2], occupation=fields[3], zip=fields[4])

    except:

        # If parsing fails, None is returned.  å¦‚æœè§£æå¤±è´¥ï¼Œåˆ™è¿”å›None
        return None

if __name__ == "__main__":
    #
    spark = SparkSession.builder \
        .appName("ScientistUsers") \
        .getOrCreate()

    # è¯»å–å¹¶è§£æç”¨æˆ·æ•°æ® Read and parse user data
    lines = spark.sparkContext.textFile("hdfs:///user/maria_dev/ml-100k/u.user")
    users = lines.map(parseUser).filter(lambda x: x is not None)
    # å®šä¹‰åˆ—å Define column names
    columns = ["userID", "age", "gender", "occupation", "zip"]
    # Convert named tuple to DataFrame. å°†å‘½åå…ƒç»„è½¬æ¢ä¸ºDataFrameã€‚
    usersDF = users.toDF(columns)
    
    # æ‰¾å‡ºèŒä¸šæ˜¯â€œscientistâ€ï¼Œå¹´é¾„åœ¨30åˆ°40å²ä¹‹é—´çš„ç”¨æˆ·ã€‚ Find users whose occupation is "scientist" and whose age is between 30 and 40.
    scientists = usersDF.filter((usersDF.occupation == "scientist") & (usersDF.age.between(30, 40)))
    
    #
    scientists.show(10)
    
    #
    spark.stop()


spark-submit fiveSQL.py

"""

from IPython.display import Image
Image('/content/v.png', width = 400)